{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba406d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read data and preprocess\n",
    "df = pd.read_csv('combined_data_9gauges.csv')\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "\n",
    "\n",
    "# Analyze rainfall events and store in a new DataFrame\n",
    "events = []\n",
    "last_event_end = {station: None for station in df['STATION'].unique()}  # Dictionary to store the end time of the last event for each station\n",
    "\n",
    "for station in df['STATION'].unique():\n",
    "    station_df = df[df['STATION'] == station]\n",
    "    event_start = None\n",
    "    event_end = None\n",
    "    total_rainfall = 0\n",
    "    max_value = 0\n",
    "    event_duration_intervals = 0\n",
    "\n",
    "    for index, row in station_df.iterrows():\n",
    "        if row['VALUE'] > 0:\n",
    "            if event_start is None:\n",
    "                event_start = row['DATETIME']\n",
    "                # Calculate intercurrence time in minutes if there was a previous event\n",
    "                intercurrence = (event_start - last_event_end[station]).total_seconds() / 60 if last_event_end[station] else 0\n",
    "            event_end = row['DATETIME']\n",
    "            total_rainfall += row['VALUE']\n",
    "            max_value = max(max_value, row['VALUE'])\n",
    "            event_duration_intervals += 1\n",
    "        else:\n",
    "            if event_start is not None:\n",
    "                duration = event_duration_intervals * 10\n",
    "                # Only append the event if its duration is greater than 0\n",
    "                if duration > 0:\n",
    "                    events.append([station, event_start, event_end, total_rainfall, duration, max_value, intercurrence])\n",
    "                last_event_end[station] = event_end  # Update the end time of the last event for this station\n",
    "                event_start = None\n",
    "                total_rainfall = 0\n",
    "                max_value = 0\n",
    "                event_duration_intervals = 0\n",
    "\n",
    "    # Check if the last record is part of an ongoing event\n",
    "    if event_start is not None:\n",
    "        duration = event_duration_intervals\n",
    "        if duration > 0:\n",
    "            events.append([station, event_start, event_end, total_rainfall, duration, max_value, intercurrence])\n",
    "\n",
    "# Create DataFrame from events list\n",
    "events_df = pd.DataFrame(events, columns=['STATION', 'START', 'END', 'TOTAL_RAINFALL', 'DURATION', 'MAX_VALUE', 'INTERCURRENCE'])\n",
    "\n",
    "# Save to CSV\n",
    "events_df.to_csv('rainfall_events.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDF\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "\n",
    "# Filter for station 706\n",
    "data = data[data['STATION'] == 706]\n",
    "\n",
    "# Filter for the time intervals\n",
    "data_2002_2012 = data[(data['START'].dt.year >= 2002) & (data['START'].dt.year <= 2012)]\n",
    "data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023)]\n",
    "\n",
    "def plot_hist_pdf_side_by_side(data1, data2, column, title, limits_dict, bin_settings,alpha=0.3, log_scale_x=True, log_scale_y=True):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(f'Catania Normalized Histograms and PDFs of {title}')\n",
    "\n",
    "    # Apply bin settings if available for the column\n",
    "    bins = bin_settings.get(column, 'auto')  # default to 'auto' if not specified\n",
    "\n",
    "    # Apply limits if available for the column\n",
    "    x_limits = limits_dict.get(column, {}).get('x')\n",
    "    y_limits = limits_dict.get(column, {}).get('y')\n",
    "\n",
    "    for i, data in enumerate([data1, data2]):\n",
    "        sns.histplot(data[column], kde=True, bins=bins, stat=\"density\", linewidth=0, ax=axes[i], alpha=alpha, kde_kws={'bw_adjust': bw_adjust})\n",
    "        axes[i].set_title(f'{title} {\"(2002-2012)\" if i == 0 else \"(2013-2023)\"}')\n",
    "        axes[i].set_xlabel(title)\n",
    "        axes[i].set_ylabel('Density')\n",
    "        if x_limits:\n",
    "            axes[i].set_xlim(x_limits)\n",
    "        if y_limits:\n",
    "            axes[i].set_ylim(y_limits)\n",
    "        if log_scale_x:\n",
    "            axes[i].set_xscale('log')\n",
    "        if log_scale_y:\n",
    "            axes[i].set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "bw_adjust = 0.5\n",
    "# Define the limits for each variable\n",
    "limits = {\n",
    "    'DURATION': {'x': [10, 300], 'y': [0.0001, 0.1]},\n",
    "    'TOTAL_RAINFALL': {'x': [1, 200], 'y': [0.0001, 1]},\n",
    "    'MAX_VALUE': {'x': [1, 31], 'y': [0.0001, 1]},\n",
    "    'INTERCURRENCE': {'x': [10, 150000], 'y': [0.00000001, 0.001]}\n",
    "}\n",
    "# Define the bin settings for each variable\n",
    "bin_settings = {\n",
    "    'DURATION': 170,         # Specify number of bins\n",
    "    'TOTAL_RAINFALL': 150,\n",
    "    'MAX_VALUE': 70,\n",
    "    'INTERCURRENCE': 300\n",
    "}\n",
    "alpha = 0.3\n",
    "\n",
    "# Plotting\n",
    "columns = ['DURATION', 'TOTAL_RAINFALL', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "for column in columns:\n",
    "    plot_hist_pdf_side_by_side(data_2002_2012, data_2013_2023, column, column, limits, bin_settings, alpha, log_scale_x=True, log_scale_y=True)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDF per city\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "\n",
    "# Filter for station 706\n",
    "data = data[data['STATION'] == 706]\n",
    "\n",
    "# Filter for the time intervals\n",
    "data_2002_2012 = data[(data['START'].dt.year >= 2002) & (data['START'].dt.year <= 2012)]\n",
    "data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023)]\n",
    "\n",
    "def plot_hist_pdf_side_by_side(data1, data2, column, title, limits_dict, bin_settings, alpha=0.3, log_scale_x=True, log_scale_y=True, save_file=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(f'Catania Normalized Histograms and PDFs of {title}')\n",
    "\n",
    "    # Apply bin settings if available for the column\n",
    "    bins = bin_settings.get(column, 'auto')  # default to 'auto' if not specified\n",
    "\n",
    "    # Apply limits if available for the column\n",
    "    x_limits = limits_dict.get(column, {}).get('x')\n",
    "    y_limits = limits_dict.get(column, {}).get('y')\n",
    "\n",
    "    for i, data in enumerate([data1, data2]):\n",
    "        sns.histplot(data[column], kde=True, bins=bins, stat=\"density\", linewidth=0, ax=axes[i], alpha=alpha, kde_kws={'bw_adjust': bw_adjust})\n",
    "        axes[i].set_title(f'{title} {\"(2002-2012)\" if i == 0 else \"(2013-2023)\"}')\n",
    "        axes[i].set_xlabel(title)\n",
    "        axes[i].set_ylabel('Density')\n",
    "        if x_limits:\n",
    "            axes[i].set_xlim(x_limits)\n",
    "        if y_limits:\n",
    "            axes[i].set_ylim(y_limits)\n",
    "        if log_scale_x:\n",
    "            axes[i].set_xscale('log')\n",
    "        if log_scale_y:\n",
    "            axes[i].set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    if save_file:\n",
    "        plt.savefig(save_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "bw_adjust = 0.5\n",
    "# Define the limits for each variable\n",
    "limits = {\n",
    "    'DURATION': {'x': [10, 300], 'y': [0.0001, 0.1]},\n",
    "    'TOTAL_RAINFALL': {'x': [1, 200], 'y': [0.0001, 1]},\n",
    "    'MAX_VALUE': {'x': [1, 31], 'y': [0.0001, 1]},\n",
    "    'INTERCURRENCE': {'x': [10, 150000], 'y': [0.00000001, 0.001]}\n",
    "}\n",
    "# Define the bin settings for each variable\n",
    "bin_settings = {\n",
    "    'DURATION': 170,         # Specify number of bins\n",
    "    'TOTAL_RAINFALL': 150,\n",
    "    'MAX_VALUE': 70,\n",
    "    'INTERCURRENCE': 300\n",
    "}\n",
    "alpha = 0.3\n",
    "\n",
    "# Plotting and saving\n",
    "columns = ['DURATION', 'TOTAL_RAINFALL', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "for column in columns:\n",
    "    save_file = f\"{column}_ct.jpg\"\n",
    "    plot_hist_pdf_side_by_side(data_2002_2012, data_2013_2023, column, column, limits, bin_settings, alpha, log_scale_x=True, log_scale_y=True, save_file=save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF comparison between stations\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "\n",
    "# Filter for the time intervals\n",
    "data_2002_2012 = data[(data['START'].dt.year >= 2002) & (data['START'].dt.year <= 2012)]\n",
    "data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023)]\n",
    "\n",
    "# Combine the two datasets for ease of processing\n",
    "combined_data = pd.concat([data_2002_2012, data_2013_2023])\n",
    "\n",
    "def plot_hist_pdf_for_stations(data_2002_2012, data_2013_2023, column, title, limits_dict, bin_settings, station_colors, alpha=0.5, log_scale_x=True, log_scale_y=True, save_file=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(30, 9))  # Two subplots for each time interval\n",
    "    fig.suptitle(f'Rainfall Analysis: {title}')\n",
    "\n",
    "    bins = bin_settings.get(column, 'auto')\n",
    "    x_limits = limits_dict.get(column, {}).get('x')\n",
    "    y_limits = limits_dict.get(column, {}).get('y')\n",
    "\n",
    "    for i, data in enumerate([data_2002_2012, data_2013_2023]):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f'{column} {\"(2002-2012)\" if i == 0 else \"(2013-2023)\"}')\n",
    "\n",
    "        for station, color in station_colors.items():\n",
    "            station_data = data[data['STATION'] == station]\n",
    "            sns.kdeplot(station_data[column], ax=ax, color=color, label=f'Station {station}', bw_adjust=bw_adjust)\n",
    "\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_ylabel('Density')\n",
    "        if x_limits:\n",
    "            ax.set_xlim(x_limits)\n",
    "        if y_limits:\n",
    "            ax.set_ylim(y_limits)\n",
    "        if log_scale_x:\n",
    "            ax.set_xscale('log')\n",
    "        if log_scale_y:\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right', title='Stations')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    if save_file:\n",
    "        plt.savefig(save_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "bw_adjust = 0.7\n",
    "\n",
    "# Define the limits for each variable\n",
    "limits = {\n",
    "    'DURATION': {'x': [10, 1000], 'y': [0.0001, 1]},\n",
    "    'TOTAL_RAINFALL': {'x': [1, 1000], 'y': [0.0001, 1]},\n",
    "    'MAX_VALUE': {'x': [1, 31], 'y': [0.0001, 1]},\n",
    "    'INTERCURRENCE': {'x': [10, 10000], 'y': [0.0001, 0.01]}\n",
    "}\n",
    "# Define the bin settings for each variable\n",
    "bin_settings = {\n",
    "    'DURATION': 170,         # Specify number of bins\n",
    "    'TOTAL_RAINFALL': 150,\n",
    "    'MAX_VALUE': 70,\n",
    "    'INTERCURRENCE': 300\n",
    "}\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "station_colors = { # Assuming you have 7 stations\n",
    "    729: 'blue',\n",
    "    764: '#9ABDDC',\n",
    "    706: '#B4CF68',\n",
    "    756: '#FFD872',\n",
    "    718: 'orange',\n",
    "    695: '#FF96C5',\n",
    "    684: '#FF00FF',\n",
    "    750: 'purple',\n",
    "    789: 'black'\n",
    "}\n",
    "\n",
    "# Plotting for each variable\n",
    "columns = ['DURATION', 'TOTAL_RAINFALL', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "for column in columns:\n",
    "    save_file = f\"rainfall_{column}_stations.jpg\"\n",
    "    plot_hist_pdf_for_stations(data_2002_2012, data_2013_2023, column, column, limits, bin_settings, \n",
    "                               station_colors, alpha, log_scale_x=True, log_scale_y=True, save_file=save_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF decades\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "\n",
    "# Filter data for the specified time intervals\n",
    "data_2002_2012 = data[(data['START'].dt.year >= 2002) & (data['START'].dt.year <= 2012)]\n",
    "data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023)]\n",
    "\n",
    "# Function to plot KDE plots for rainfall data comparison\n",
    "def plot_kde(data_2002_2012, data_2013_2023, column, title, bw_adjust_2002_2012, bw_adjust_2013_2023, y_limits=None, log_scale_x=True, log_scale_y=True, save_file=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(data_2002_2012[column], color='blue', label='2002-2012', bw_adjust=bw_adjust_2002_2012)\n",
    "    sns.kdeplot(data_2013_2023[column], color='red', label='2013-2023', bw_adjust=bw_adjust_2013_2023)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Density')\n",
    "    if y_limits:\n",
    "        plt.ylim(y_limits)\n",
    "    if log_scale_x:\n",
    "        plt.xscale('log')\n",
    "    if log_scale_y:\n",
    "        plt.yscale('log')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_file:\n",
    "        plt.savefig(save_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plotting parameters\n",
    "columns = ['DURATION', 'TOTAL_RAINFALL', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "bandwidths_2002_2012 = {\n",
    "    'DURATION': 130,  # Adjust these values as needed for smoothing\n",
    "    'TOTAL_RAINFALL': 130,\n",
    "    'MAX_VALUE': 13,\n",
    "    'INTERCURRENCE': 13\n",
    "}\n",
    "bandwidths_2013_2023 = {\n",
    "    'DURATION': 130,\n",
    "    'TOTAL_RAINFALL': 130,\n",
    "    'MAX_VALUE': 13,\n",
    "    'INTERCURRENCE': 13\n",
    "}\n",
    "\n",
    "y_limits = {\n",
    "    'DURATION': (0.00000001, 1),\n",
    "    'TOTAL_RAINFALL': (0.00000001, 1),\n",
    "    'MAX_VALUE': (0.00000001, 1),\n",
    "    'INTERCURRENCE': (0.00000001, 0.0001)\n",
    "}\n",
    "\n",
    "# Plotting for each variable\n",
    "for column in columns:\n",
    "    save_file = f\"rainfall_{column}_comparison.jpg\"\n",
    "    plot_kde(data_2002_2012, data_2013_2023, column, column, \n",
    "             bw_adjust_2002_2012=bandwidths_2002_2012[column], \n",
    "             bw_adjust_2013_2023=bandwidths_2013_2023[column], \n",
    "             y_limits=y_limits[column], \n",
    "             log_scale_x=True, log_scale_y=True, \n",
    "             save_file=save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61533f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increments comparison between stations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "\n",
    "# Define the columns for which you want to calculate simple returns\n",
    "columns = ['DURATION', 'TOTAL_RAINFALL', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "\n",
    "def calculate_normalized_simple_returns(data, column_name):\n",
    "    # Calculate simple returns\n",
    "    data['SIMPLE_RETURNS'] = data[column_name].diff()\n",
    "    # Normalize simple returns\n",
    "    mean = data['SIMPLE_RETURNS'].mean()\n",
    "    std_dev = data['SIMPLE_RETURNS'].std()\n",
    "    data['NORMALIZED_SIMPLE_RETURNS'] = (data['SIMPLE_RETURNS'] - mean) / std_dev\n",
    "    return data\n",
    "\n",
    "# Plotting function for normalized simple returns with explicit axis limits\n",
    "def plot_normalized_simple_returns(data, column_name, station_colors, x_limits=None, y_limits=None, save_file=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(f'Normalized Simple Returns of {column_name}')\n",
    "\n",
    "    for time_period, ax in zip(['2002-2012', '2013-2023'], axes):\n",
    "        time_data = data[(data['START'].dt.year >= int(time_period.split('-')[0])) & (data['START'].dt.year <= int(time_period.split('-')[1]))]\n",
    "        ax.set_title(f'{column_name} ({time_period})')\n",
    "        ax.set_xlabel('Normalized Simple Returns')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        for station in time_data['STATION'].unique():\n",
    "            station_data = calculate_normalized_simple_returns(time_data[time_data['STATION'] == station].copy(), column_name)\n",
    "            sns.kdeplot(station_data['NORMALIZED_SIMPLE_RETURNS'].dropna(), ax=ax, color=station_colors.get(station, 'grey'), label=f'Station {station}', bw_adjust=0.5)\n",
    "        \n",
    "        # Explicitly set x and y limits if provided\n",
    "        if x_limits:\n",
    "            ax.set_xlim(x_limits)\n",
    "        if y_limits:\n",
    "            ax.set_ylim(y_limits)\n",
    "        \n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_file:\n",
    "        plt.savefig(save_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "bw_adjust = 31 \n",
    "\n",
    "# Define x-axis limits for specific columns\n",
    "x_limits = {\n",
    "    'DURATION': [-8, 8],\n",
    "    'TOTAL_RAINFALL': [-15, 15],\n",
    "    'MAX_VALUE': [-10, 10], \n",
    "    'INTERCURRENCE': [-15, 15]\n",
    "}\n",
    "\n",
    "# Define y-axis limits for specific columns\n",
    "y_limits = {\n",
    "    'DURATION': [0.001, 3],\n",
    "    'TOTAL_RAINFALL': [0.001, 3],\n",
    "    'MAX_VALUE': [0.001, 3], \n",
    "    'INTERCURRENCE': [0.001, 3]\n",
    "}\n",
    "\n",
    "# Define station colors\n",
    "station_colors = { \n",
    "    729: 'blue',\n",
    "    764: '#9ABDDC',\n",
    "    706: '#B4CF68',\n",
    "    756: '#FFD872',\n",
    "    718: 'orange',\n",
    "    695: '#FF96C5',\n",
    "    684: '#FF00FF',\n",
    "    750: 'purple',\n",
    "    789: 'black'\n",
    "}\n",
    "\n",
    "# Loop through columns and plot\n",
    "for column in columns:\n",
    "    save_file = f\"{column}_returns.jpg\"\n",
    "    plot_normalized_simple_returns(data, column, station_colors, x_limits.get(column), y_limits.get(column), save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f18631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit PDF\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Power-law function\n",
    "def power_law(x, a, b):\n",
    "    return a * np.power(x, -b)\n",
    "\n",
    "# Function to plot the fitted power-law curve\n",
    "def plot_fit(ax, x, y, a, b, color):\n",
    "    x_fit = np.logspace(np.log10(np.min(x)), np.log10(np.max(x)), 100)\n",
    "    y_fit = power_law(x_fit, a, b)\n",
    "    ax.plot(x_fit, y_fit, color=color, linewidth=1, linestyle='-',  label=f'Power law b=-{b:.2f}')\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime and apply other filters\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "data = data[data['STATION'] != 729]\n",
    "columns_to_filter = ['TOTAL_RAINFALL', 'DURATION', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "data = data[(data[columns_to_filter] > 0.1).all(axis=1)]\n",
    "\n",
    "#data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023)]\n",
    "data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023) & \n",
    "                      (data['START'].dt.month.isin([9, 10, 11]))]\n",
    "\n",
    "# Setting bins for DURATION using logspace\n",
    "min_duration = data_2013_2023['MAX_VALUE'].min()\n",
    "max_duration = data_2013_2023['MAX_VALUE'].max()\n",
    "log_bins_duration = np.logspace(np.log10(min_duration), np.log10(max_duration), 50)\n",
    "\n",
    "# Histogram plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Calculate histogram with log bins\n",
    "values, base = np.histogram(data_2013_2023['MAX_VALUE'], bins=log_bins_duration, density=True)\n",
    "counts, _ = np.histogram(data_2013_2023['MAX_VALUE'], bins=log_bins_duration)\n",
    "\n",
    "# Calculate bin centers in log space\n",
    "bin_centers = 10**(0.5 * (np.log10(base[:-1]) + np.log10(base[1:])))\n",
    "\n",
    "# Filter bins where count > 50\n",
    "filtered_centers = bin_centers[counts > 100]\n",
    "filtered_values = values[counts > 100]\n",
    "\n",
    "# Plot filtered bins\n",
    "ax.plot(filtered_centers, filtered_values, color='mediumseagreen', marker='o', markersize=7, linestyle='', label='Max per event (2013-2023)')\n",
    "\n",
    "ax.set_xlabel('Max per event (mm)',fontsize='large')\n",
    "ax.set_ylabel('Probability Density ',fontsize='large')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0,4)\n",
    "#ax.set_ylim(0.01,11)\n",
    "ax.grid(False)\n",
    "\n",
    "# Fit to power-law and plot\n",
    "p0_2013_2023 = [.1, -2]  # Example initial parameters, adjust as needed\n",
    "params, _ = curve_fit(power_law, filtered_centers, filtered_values, p0=p0_2013_2023)\n",
    "plot_fit(ax, filtered_centers, filtered_values, params[0], params[1], 'red')\n",
    "\n",
    "ax.legend(fontsize='x-large')\n",
    "plt.tight_layout()\n",
    "plt.savefig('4.fithistogram_max_2013_2023.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fti decumulative probability\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import expi\n",
    "\n",
    "# Q-Exponential function\n",
    "def q_exponential(x, A, q, beta):\n",
    "    return A * np.exp(-beta * x ** q) / expi(q)\n",
    "\n",
    "# Function to plot the fitted Q-exponential curve\n",
    "def plot_fit(ax, x, y, A, q, beta, color):\n",
    "    x_fit = np.logspace(np.log10(np.min(x)), np.log10(np.max(x)), 100)\n",
    "\n",
    "    y_fit = q_exponential(x_fit, A, q, beta)\n",
    "    ax.plot(x_fit, y_fit, color=color, linewidth=1, linestyle='-', label=f'Q-Exponential q={q:.2f}')\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime and apply other filters\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "data = data[data['STATION'] != 729]\n",
    "columns_to_filter = ['TOTAL_RAINFALL', 'DURATION', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "data = data[(data[columns_to_filter] > 0.1).all(axis=1)]\n",
    "\n",
    "#data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023)]\n",
    "data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023) & \n",
    "                      (data['START'].dt.month.isin([9, 10, 11]))]\n",
    "\n",
    "\n",
    "# Setting bins for DURATION using linear space\n",
    "min_duration = data_2013_2023['DURATION'].min()\n",
    "max_duration = data_2013_2023['DURATION'].max()\n",
    "#bins_duration = np.logspace(np.log10(min_duration), np.log10(max_duration), 30)\n",
    "bins_duration = 300\n",
    "# Histogram plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Calculate histogram with linear bins\n",
    "values, bins = np.histogram(data_2013_2023['DURATION'], bins=bins_duration, density=True)\n",
    "\n",
    "# Calculate cumulative probabilities for the x-axis\n",
    "cdf_values = np.cumsum(values)\n",
    "\n",
    "# Calculate decumulative probabilities\n",
    "decumulative_values = 1 - cdf_values / np.sum(values)\n",
    "\n",
    "# Plot histogram\n",
    "ax.plot(bins[:-1], decumulative_values, color='mediumseagreen', marker='o', markersize=7, linestyle='', \n",
    "        label='Duration event (2013-2023)')\n",
    "\n",
    "ax.set_xlabel('Duration event Autumn')\n",
    "ax.set_ylabel('Decumulative Probability')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.00011,10)\n",
    "ax.grid(False)\n",
    "\n",
    "# Fit to Q-exponential and plot\n",
    "p0_2013_2023 = [0.09, 1.1, 0.09]  # Example initial parameters, adjust as needed\n",
    "params, _ = curve_fit(q_exponential, bins[:-1], decumulative_values, p0=p0_2013_2023, maxfev=80000)\n",
    "plot_fit(ax, bins[:-1], decumulative_values, params[0], params[1], params[2], 'red')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('4.fitdec_duration_2013_2023.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be95232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Tsallis Q-Exponential function\n",
    "def q_exponential(x, A, q, beta):\n",
    "    return A * (1 - (1 - q) * beta * x) ** (1 / (1 - q))\n",
    "\n",
    "# Load data from a file. Assume the first row contains column names.\n",
    "data = pd.read_csv('rainfall_events.csv', header=0, parse_dates=['START', 'END'])\n",
    "\n",
    "# Filter data to include only events from December, January, and February between 2002 and 2012\n",
    "filtered_data = data[(data['START'].dt.month.isin([12, 1, 2])) & \n",
    "                     (data['START'].dt.year >= 2002) & \n",
    "                     (data['START'].dt.year <= 2012)]\n",
    "\n",
    "# Accessing the \"TOTAL_RAINFALL\" column from the filtered data\n",
    "values = filtered_data['TOTAL_RAINFALL'].values\n",
    "\n",
    "# Define the range of threshold values\n",
    "thresholds = np.arange(.1, 800.1, .1)\n",
    "\n",
    "# Calculate the fraction of values greater than each threshold\n",
    "fractions = [(values > threshold).mean() for threshold in thresholds]\n",
    "\n",
    "# Parameters for the Tsallis Q-Exponential function (example values, adjust as needed)\n",
    "A = 1 # Scale factor\n",
    "q = 1.73 # Entropy index\n",
    "beta = 2.4 # Scale parameter\n",
    "\n",
    "# Calculate Tsallis Q-Exponential values for the thresholds\n",
    "tsallis_values = q_exponential(thresholds, A, q, beta)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, fractions, 'o', markersize=4, label='Depth (2002-2012)', color='lightskyblue')  # Added 'b^' for blue triangles\n",
    "plt.plot(thresholds, tsallis_values, label=f'q={q:.2f} k={beta:.2f}', linestyle='--',linewidth=.8, color='red')\n",
    "plt.xlabel('Depth (mm)',fontsize='large')\n",
    "plt.ylabel('Decumuative probability',fontsize='large')\n",
    "plt.ylim(0.01, 2)\n",
    "plt.xlim(0.1,100)\n",
    "plt.xscale('log')  # Using logarithmic scale for x-axis\n",
    "plt.yscale('log')  # Using logarithmic scale for y-axis\n",
    "plt.legend(fontsize='x-large')\n",
    "#plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.grid(False) \n",
    "plt.savefig('1.fit_decumulative_depth_2002_2012.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit frequency vs ranking \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "file_path = 'rainfall_events.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert START and END to datetime and apply other filters\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "data = data[data['STATION'] != 729]\n",
    "columns_to_filter = ['TOTAL_RAINFALL', 'DURATION', 'MAX_VALUE', 'INTERCURRENCE']\n",
    "data = data[(data[columns_to_filter] > 0.1).all(axis=1)]\n",
    "\n",
    "#data_2013_2023 = data[(data['START'].dt.year >= 2002) & (data['START'].dt.year <= 2012)]\n",
    "data_2013_2023 = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023) & \n",
    "                      (data['START'].dt.month.isin([3,4, 5]))]\n",
    "\n",
    "\n",
    "# Rank the durations by frequency and normalize the counts\n",
    "duration_counts = data_2013_2023['INTERCURRENCE'].value_counts(normalize=True)\n",
    "duration_ranks = np.arange(1, len(duration_counts) + 1)\n",
    "\n",
    "# Plot frequency against rank\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.loglog(duration_ranks, duration_counts.values, marker='o', linestyle='', color='blue')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Frequency (Normalized)')\n",
    "plt.title('Zipf\\'s Law - Duration')\n",
    "plt.grid(True)\n",
    "\n",
    "# Fit a line to the data\n",
    "slope, intercept = np.polyfit(np.log(duration_ranks), np.log(duration_counts.values), 1)\n",
    "plt.plot(duration_ranks, np.exp(intercept) * np.power(duration_ranks, slope), color='red', label=f'Fit (slope={slope:.2f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('zipf_duration_fit_normalized.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit increments\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the Tsallis Q-Gaussian function\n",
    "def q_gaussian(x, A, q, beta, mu):\n",
    "    term = 1 - (1 - q) * beta * (x - mu)**2\n",
    "    valid = term > 0\n",
    "    result = np.zeros_like(x)\n",
    "    if valid.any():\n",
    "        result[valid] = A * np.power(term[valid], 1 / (1 - q))\n",
    "    return result\n",
    "\n",
    "# Load and preprocess data\n",
    "file_path = 'rainfall_events.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data['START'] = pd.to_datetime(data['START'])\n",
    "data['END'] = pd.to_datetime(data['END'])\n",
    "\n",
    "# Filter data by years (2002 to 2012) and remove station 729\n",
    "#data = data[(data['START'].dt.year >= 2002) & (data['START'].dt.year <= 2012)]\n",
    "data = data[(data['START'].dt.year >= 2013) & (data['START'].dt.year <= 2023) & \n",
    "            (data['START'].dt.month.isin([6, 7, 8]))]\n",
    "#data = data[data['STATION'] != 729]\n",
    "\n",
    "# Function to calculate normalized simple returns\n",
    "def calculate_normalized_simple_returns(data, column_name):\n",
    "    data['SIMPLE_RETURNS'] = data[column_name].diff()\n",
    "    mean = data['SIMPLE_RETURNS'].mean()\n",
    "    std_dev = data['SIMPLE_RETURNS'].std()\n",
    "    data['NORMALIZED_SIMPLE_RETURNS'] = (data['SIMPLE_RETURNS'] - mean) / std_dev\n",
    "    return data\n",
    "\n",
    "# Combine data from all stations\n",
    "combined_data = pd.concat([data[data['STATION'] == station] for station in data['STATION'].unique()])\n",
    "combined_data = calculate_normalized_simple_returns(combined_data, 'INTERCURRENCE')\n",
    "\n",
    "# Parameters for the Tsallis Q-Gaussian function (example values, adjust as needed)\n",
    "A = 2.2  # Amplitude\n",
    "q = 2.45\n",
    "beta = 2500 # Scale parameter\n",
    "mu = 0  # Mean\n",
    "\n",
    "# Generate values for the Tsallis Q-Gaussian\n",
    "x_values = np.linspace(-13, 13, 400)\n",
    "y_values = q_gaussian(x_values, A, q, beta, mu)\n",
    "\n",
    "# Plotting 250bins\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#fig.suptitle('Max Simple Returns')\n",
    "sns.histplot(combined_data['NORMALIZED_SIMPLE_RETURNS'].dropna(), bins=200, \n",
    "             kde=False, ax=ax, color='mediumseagreen', label='Interevents (2013-2023)')\n",
    "ax.plot(x_values, y_values * len(combined_data['NORMALIZED_SIMPLE_RETURNS'].dropna()) * (16 / 30), color='red', label=f'q={q:.2f}, β={beta:.2f}')  # Scale curveax.set_xlabel('Normalized Simple Returns', fontsize='large')\n",
    "ax.set_ylabel('Probability Density', fontsize='large')\n",
    "ax.set_xlabel('Normalized simple returns', fontsize='large')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(-4,4)\n",
    "ax.set_ylim(0.9, 50000)\n",
    "ax.legend(fontsize='x-large')  # Enlarged legend\n",
    "plt.tight_layout()\n",
    "plt.savefig('3.returns_inter_2013_2023.jpg', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
